<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="pragma" content="no-cache">
  <meta http-equiv="cache-control" content="no-cache">
  <meta http-equiv="expires" content="0">
  
  <title>爬虫实战经历 | warmeng</title>
  <meta name="author" content="Mr.w">
  
  <meta name="description" content="毕业的东西总算全部搞完了，有闲情来弄博客了，毕业还挺伤感的。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="爬虫实战经历"/>
  <meta property="og:site_name" content="warmeng"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-70812759-1', 'auto');
  ga('send', 'pageview');
</script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?cb5448498d7169c668b07c2b255d62c1";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">warmeng</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="所有文章的匯總.">
			  <i class=""></i>歸檔
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="所有文章的分類.">
			  <i class=""></i>分類
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="所有文件的標籤.">
			  <i class=""></i>標籤
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="新垣結衣的小跟班.">
			  <i class=""></i>關於我
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 


	
		<div class="page-header">
			<h1> 爬虫实战经历</h1>
		</div>
	



<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>毕业的东西总算全部搞完了，有闲情来弄博客了，毕业还挺伤感的。</p>
<a id="more"></a>
<h1 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h1><p>由老师给的一个兼职任务，对户型图网站的图片的抓取，整个端午节都在研究这个东西，总算是弄出来了。</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>目标网站为<a href="https://www.kujiale.com/huxing/reside/,&quot;title&quot;" target="_blank" rel="external">户型图</a> 。<br>打开robots.txt发现目标网站的网站地图。<a href="http://www.kujiale.com/sitemap_index.xml" target="_blank" rel="external">网站地图</a> 。<br>根据网站地图收集所需要的url。根据下面代码爬取到所有的loc标签，通过标签下载下来所有的loc的url。<br>使用subline通过正则表达式find出所有带有huxing/hangzhou的url。<br>使用脚本对每个url进行访问。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment"># !/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line">import threading</div><div class="line">import random</div><div class="line">import time</div><div class="line">import requests</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line"></div><div class="line">threads = []</div><div class="line">request = requests.session()</div><div class="line">header = &#123;&#125;</div><div class="line">header[<span class="string">"Host"</span>] = <span class="string">"www.kujiale.com"</span></div><div class="line">header[<span class="string">"User-Agent"</span>] = <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0"</span></div><div class="line">header[<span class="string">"Accept"</span>] = <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"</span></div><div class="line">header[<span class="string">"Accept-Language"</span>] = <span class="string">"zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3"</span></div><div class="line">header[<span class="string">"Accept-Encoding"</span>] = <span class="string">"gzip, deflate, br"</span></div><div class="line">header[<span class="string">"Cookie"</span>] = <span class="string">"KSESSIONID=d98ab2aa-8e73-4475-a1d3-b6b9ec32131c; gr_user_id=2c1c9985-db0d-4631-bfcf-9badbfd90ff7; Hm_lvt_bd8fd4c378d7721976f466053bd4a855=1495932728,1496019873; _ga=GA1.2.2058694673.1495932730; _gid=GA1.2.1642389268.1496019877; hasShownFootAd=true; kjl_sessionid=7843d03a-05e9-41eb-acaa-fc745a0c29a1; qqconn_access_token=571959B96EAFD7B00EF59A830D5A04FA; qqconn_openid=7F1637AF553192C0210D62BD83266049; qhssokey=3FO4K9RIQKVTVK7O138C; qhssokeyid=VK7O138C; qhssokeycheck=3FO4K9RIQKVT; 2017-05-28-sign-3FO4K9RIQKVT=false; landingpageurl=http://www.kujiale.com/huxing/reside/; 2017-05-29-sign-3FO4K9RIQKVT=false; kjl_usercityid=175; gr_session_id_a4a13a22eb51522b=b58ac865-047b-4767-a85d-b50198e13b9b; gr_cs1_b58ac865-047b-4767-a85d-b50198e13b9b=userId%3A3FO4K9RIQKVT; JSESSIONID=1o61l8us4a7rwtfvyj8mntc5y; Hm_lpvt_bd8fd4c378d7721976f466053bd4a855=1496019877"</span></div><div class="line">header[<span class="string">"Connection"</span>] = <span class="string">"keep-alive"</span></div><div class="line"></div><div class="line"></div><div class="line">def dowloadPic(imageUrl, filePath):</div><div class="line">    r = requests.get(imageUrl)</div><div class="line">    with open(filePath, <span class="string">"wb"</span>) as code:</div><div class="line">        code.write(r.content)</div><div class="line">indexUrl = <span class="string">"https://www.kujiale.com/sitemap_index.xml"</span></div><div class="line"></div><div class="line"></div><div class="line">results = request.get(indexUrl, headers=header)<span class="comment">#进入1级地图并且遍历</span></div><div class="line">detail = str(BeautifulSoup(results.content,<span class="string">"lxml"</span>))</div><div class="line">links = re.findall(<span class="string">'&lt;loc&gt;(.*?)&lt;/loc&gt;'</span>,detail)</div><div class="line"><span class="keyword">for</span> link <span class="keyword">in</span> links:</div><div class="line">  link2 = str(link)</div><div class="line">  results2 = request.get(link2,headers=header)<span class="comment">#2级地图并且遍历</span></div><div class="line">  detail2 = str(BeautifulSoup(results2.content,<span class="string">'lxml'</span>))</div><div class="line">  jieguos = re.findall(<span class="string">'&lt;loc&gt;(.*?)&lt;/loc&gt;'</span>,detail2)</div><div class="line">  <span class="keyword">for</span> jieguo <span class="keyword">in</span> jieguos:</div><div class="line">      jieguo2 = str(jieguo)</div><div class="line">      <span class="built_in">print</span> jieguo2</div><div class="line">      w = open(<span class="string">"/Users/nevermore/Desktop/HUXINGTU/666.txt"</span>, <span class="string">"a+"</span>)</div><div class="line">      w.write(jieguo2 + <span class="string">'\r\n'</span>)</div></pre></td></tr></table></figure></p>
<h2 id="信息筛选"><a href="#信息筛选" class="headerlink" title="信息筛选"></a>信息筛选</h2><p>筛选的的结果如图所示:<img src="/images/20170612/1.png" alt="">，然后讲所筛选的url进行访问，通过正则找到带有目标的jpg url<br>然后讲其下载，如下面代码所示<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># !/usr/bin/env python</span></div><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line">import threading</div><div class="line">import requests</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import os</div><div class="line">import random</div><div class="line">threads = []</div><div class="line">request = requests.session()</div><div class="line">header = &#123;&#125;</div><div class="line">header[<span class="string">"Host"</span>] = <span class="string">"www.kujiale.com"</span></div><div class="line">header[<span class="string">"User-Agent"</span>] = <span class="string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0"</span></div><div class="line">header[<span class="string">"Accept"</span>] = <span class="string">"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"</span></div><div class="line">header[<span class="string">"Accept-Language"</span>] = <span class="string">"zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3"</span></div><div class="line">header[<span class="string">"Accept-Encoding"</span>] = <span class="string">"gzip, deflate, br"</span></div><div class="line">header[<span class="string">"Cookie"</span>] = <span class="string">"KSESSIONID=d98ab2aa-8e73-4475-a1d3-b6b9ec32131c; gr_user_id=2c1c9985-db0d-4631-bfcf-9badbfd90ff7; Hm_lvt_bd8fd4c378d7721976f466053bd4a855=1495932728,1496019873,1496075232,1496075460; _ga=GA1.2.2058694673.1495932730; _gid=GA1.2.194016103.1496105563; qqconn_access_token=571959B96EAFD7B00EF59A830D5A04FA; qqconn_openid=7F1637AF553192C0210D62BD83266049; qhssokey=3FO4K9RIQKVT7QT1GWWPW; qhssokeyid=7QT1GWWPW; qhssokeycheck=3FO4K9RIQKVT; kjl_sessionid=60u1zks6yn631tkr5yk8kwn9p; Hm_lvt_55cf859f19ff9efb2389c232abf347a6=1496072964; UM_distinctid=15c54e5c72e477-03018d1179e9b4-49526a-fa000-15c54e5c72f247; _jzqa=1.4592844170854785500.1496072964.1496072964.1496072964.1; _jzqckmp=1; 2017-05-30-sign-3FO4K9RIQKVT=false; CNZZDATA1000449964=260878689-1496072838-%7C1496072838; Hm_lpvt_bd8fd4c378d7721976f466053bd4a855=1496105563; JSESSIONID=kvqb4k8qggoe1p8yxprk63yxj; gr_session_id_a4a13a22eb51522b=b707aa5c-e8f7-4d84-8813-e94c309b1f7c; gr_cs1_b707aa5c-e8f7-4d84-8813-e94c309b1f7c=userId%3A3FO4K9RIQKVT; DIYSERVERS=1"</span></div><div class="line">header[<span class="string">"Connection"</span>] = <span class="string">"keep-alive"</span></div><div class="line"></div><div class="line"></div><div class="line">def dowloadPic(imageUrl, filePath):</div><div class="line">    try:</div><div class="line">        r = requests.get(imageUrl)</div><div class="line">        with open(filePath, <span class="string">"wb"</span>) as code:</div><div class="line">            code.write(r.content)</div><div class="line">    except:</div><div class="line">        pass</div><div class="line">f = open(<span class="string">"/Users/nevermore/Desktop/HUXINGTU/hangzhou1.txt"</span>, <span class="string">"r"</span>)<span class="comment">#打开本地杭州url</span></div><div class="line">lines = f.readlines()</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> lines:</div><div class="line">    indexUrl = line</div><div class="line">    results = request.get(line, headers=header)</div><div class="line">    titlename = str(BeautifulSoup(results.content).title)</div><div class="line">    titlename = titlename[7:-7]</div><div class="line">    name = line[30:]+str(titlename)</div><div class="line">    <span class="built_in">print</span> name</div><div class="line">    try:</div><div class="line">        results = request.get(indexUrl, headers=header)</div><div class="line">        detail = str(BeautifulSoup(results.content,<span class="string">"lxml"</span>))</div><div class="line">    except:</div><div class="line">        pass</div><div class="line"></div><div class="line">    tupians = re.findall(r<span class="string">'src="https://(.*).jpg@!480w"'</span>,detail)<span class="comment">#找到带有jpg的url</span></div><div class="line">    <span class="keyword">if</span> len(tupians)!=0:</div><div class="line">        <span class="keyword">if</span> os.path.exists(<span class="string">"/Users/nevermore/Desktop/HUXINGTU/"</span> + name.strip()):<span class="comment">#判断分类文件夹是否存在</span></div><div class="line">            pass</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            os.mkdir(<span class="string">"/Users/nevermore/Desktop/HUXINGTU/"</span> + name.strip())<span class="comment">#创建分类的文件夹</span></div><div class="line">            file = open(<span class="string">"/Users/nevermore/Desktop/HUXINGTU/"</span> + name.strip() + <span class="string">"/2.txt"</span>, <span class="string">'w'</span>)<span class="comment">#创建分类文件下的2.txt</span></div><div class="line">            file.close()</div><div class="line">        <span class="keyword">for</span> tupian <span class="keyword">in</span> tupians:</div><div class="line">          tupian = <span class="string">'https://'</span> + str(tupian) + <span class="string">'.jpg'</span></div><div class="line"></div><div class="line">          path = <span class="string">'/Users/nevermore/Desktop/HUXINGTU/'</span>+name.strip()+<span class="string">'/'</span>+str(random.randint(0,1000000)) +<span class="string">'.jpg'</span></div><div class="line"></div><div class="line">          <span class="built_in">print</span> tupian</div><div class="line"></div><div class="line">          dowloadPic(tupian,path)</div></pre></td></tr></table></figure></p>
<p>开始批量的下载，在寝室下载因为网络问题，反而没有被反爬监测出来，跑到学校的电脑上爬，1分钟就被监测出来了。<br>只好加个sleep了。下载的东西如下图所示:<img src="/images/20170612/2.png" alt="">，<img src="/images/20170612/3.png" alt="">。<br>这里只取少量，所有的杭州户型图已经全部爬取完毕在学校的电脑里。</p>
<h2 id="總結"><a href="#總結" class="headerlink" title="總結"></a>總結</h2><p>毕业设计就是爬虫和反爬虫，现在又正经的玩了一遍，整个过程中遇到了很多的问题和麻烦但也乐在其中。</p>
	  
	</div>

	<div>
  	<center>
	<div class="pagination">

    
    
    <a type="button" class="btn btn-default disabled"><i class="fa fa-arrow-circle-o-left"></i>上一頁</a>
    

    <a href="/" type="button" class="btn btn-default"><i class="fa fa-home"></i>Home</a>
    
    <a href="/2017/04/19/r&l/" type="button" class="btn btn-default ">下一頁<i
                class="fa fa-arrow-circle-o-right"></i></a>
    

    
</div>

    </center>
	</div>
	
	<!-- comment -->
	
<section id="comment">
    <h2 class="title">留言</h2>

    
    <div class="ds-thread" data-thread-key="2017/06/09/spider/" data-title="爬虫实战经历"
         data-url="http://www.warmeng.com/2017/06/09/spider/"></div>
    
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2017-06-09 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/实战过程/">实战过程<span>2</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/实战/">实战<span>2</span></a></li> <li><a href="/tags/python/">python<span>1</span></a></li>
    </ul>
	</div>
		

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->

<script type="text/javascript">
  var duoshuoQuery = { short_name: 'true' };
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';
    ds.async = true;
    ds.src = 'http://static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
</script>


<span id="busuanzi_container_page_pv">
   本文总阅读量<span id="busuanzi_value_page_pv"></span>    次
</span>
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2017 Mr.w
  

  <!--
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a>,<a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>,<a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a> and <a href="http://getbootstrap.com/" target="_blank">BOOTSTRA.386</a>. 
     <br> Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind.386</a>.    -->
</p>
<span id="busuanzi_container_site_uv"> 
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span> </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>⬆︎TOP</span>
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>